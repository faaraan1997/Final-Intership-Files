{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 score: 0.10131848890407362\n",
      "Average ROUGE-2 score: 0.0\n",
      "Average ROUGE-L score: 0.10062237361585308\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load JSON data\n",
    "input_file = 'results_from_my_lora.json'  # Adjust the path to your JSON file\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Remove newline characters from the predicted_response fields\n",
    "for item in data:\n",
    "    item[\"predicted_response\"] = item[\"predicted_response\"].replace(\"\\n\", \"\")\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(predicted_text, ground_truth_text):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predicted_text, ground_truth_text)\n",
    "    rouge_1_score = scores[0][\"rouge-1\"][\"f\"]\n",
    "    rouge_2_score = scores[0][\"rouge-2\"][\"f\"]\n",
    "    rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n",
    "    return rouge_1_score, rouge_2_score, rouge_l_score\n",
    "\n",
    "# Initialize list to store ROUGE scores\n",
    "rouge_scores = []\n",
    "\n",
    "# Iterate over the data and calculate ROUGE scores\n",
    "for entry in data:\n",
    "    question = entry['question']\n",
    "    original_response = entry['original_response']\n",
    "    predicted_response = entry['predicted_response']\n",
    "    \n",
    "    rouge_1, rouge_2, rouge_l = calculate_rouge(predicted_response, original_response)\n",
    "    rouge_scores.append({\n",
    "        'question': question,\n",
    "        'original_response': original_response,\n",
    "        'predicted_response': predicted_response,\n",
    "        'ROUGE-1': rouge_1,\n",
    "        'ROUGE-2': rouge_2,\n",
    "        'ROUGE-L': rouge_l\n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "rouge_df = pd.DataFrame(rouge_scores)\n",
    "\n",
    "# Calculate average ROUGE scores\n",
    "average_rouge_scores = rouge_df[['ROUGE-1', 'ROUGE-2', 'ROUGE-L']].mean()\n",
    "\n",
    "# Print average ROUGE scores\n",
    "print(\"Average ROUGE-1 score:\", average_rouge_scores['ROUGE-1'])\n",
    "print(\"Average ROUGE-2 score:\", average_rouge_scores['ROUGE-2'])\n",
    "print(\"Average ROUGE-L score:\", average_rouge_scores['ROUGE-L'])\n",
    "\n",
    "# Optionally, save the DataFrame with ROUGE scores to a new JSON file\n",
    "# output_file = '/path/to/results_with_rouge_scores.json'  # Adjust the path to your output JSON file\n",
    "# rouge_df.to_json(output_file, orient='records', indent=4)\n",
    "# print(f\"Results with ROUGE scores saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14d3ac5e6dd44bcbfc5e2af433aef34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Scores:\n",
      "Average Cosine Similarity: 0.3780\n",
      "Average BLEU Score: 13.2176\n",
      "Average CIDEr Score: 0.5246\n",
      "Average ROUGE-1 Score: 0.4652\n",
      "Average ROUGE-2 Score: 0.2206\n",
      "Average ROUGE-L Score: 0.3527\n",
      "Average METEOR Score: 0.3457\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sacrebleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('Q_A_with_predictions_and_rouge.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract original and predicted answers\n",
    "original_answers = [item['Original_Answer'] for item in data]\n",
    "predicted_answers = [item['Predicted_Answer'] for item in data]\n",
    "\n",
    "# Cosine Similarity\n",
    "vectorizer = TfidfVectorizer().fit_transform(original_answers + predicted_answers)\n",
    "vectors = vectorizer.toarray()\n",
    "cosine_scores = [cosine_similarity([vectors[i]], [vectors[i + len(original_answers)]])[0][0] for i in range(len(original_answers))]\n",
    "\n",
    "# BLEU Score\n",
    "bleu_scores = [sacrebleu.sentence_bleu(pred, [orig]).score for orig, pred in zip(original_answers, predicted_answers)]\n",
    "\n",
    "# CIDEr Score\n",
    "cider = Cider()\n",
    "cider_scores, _ = cider.compute_score({i: [pred] for i, pred in enumerate(predicted_answers)}, \n",
    "                                       {i: [orig] for i, orig in enumerate(original_answers)})\n",
    "\n",
    "# ROUGE Scores\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for orig, pred in zip(original_answers, predicted_answers):\n",
    "    scores = rouge.score(orig, pred)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# METEOR Score using Hugging Face's evaluate library\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "meteor_scores = meteor.compute(predictions=predicted_answers, references=original_answers)\n",
    "\n",
    "# Calculate overall scores\n",
    "overall_cosine_similarity = sum(cosine_scores) / len(cosine_scores)\n",
    "overall_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "overall_cider_score = cider_scores  # Directly use the CIDEr score\n",
    "overall_rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n",
    "overall_rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n",
    "overall_rougeL_score = sum(rougeL_scores) / len(rougeL_scores)\n",
    "overall_meteor_score = meteor_scores['meteor']  # Use the METEOR score\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Scores:\")\n",
    "print(f\"Average Cosine Similarity: {overall_cosine_similarity:.4f}\")\n",
    "print(f\"Average BLEU Score: {overall_bleu_score:.4f}\")\n",
    "print(f\"Average CIDEr Score: {overall_cider_score:.4f}\")\n",
    "print(f\"Average ROUGE-1 Score: {overall_rouge1_score:.4f}\")\n",
    "print(f\"Average ROUGE-2 Score: {overall_rouge2_score:.4f}\")\n",
    "print(f\"Average ROUGE-L Score: {overall_rougeL_score:.4f}\")\n",
    "print(f\"Average METEOR Score: {overall_meteor_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\faara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Scores:\n",
      "Average Cosine Similarity: 0.1279\n",
      "Average BLEU Score: 0.2882\n",
      "Average CIDEr Score: 0.0071\n",
      "Average ROUGE-1 Score: 0.0813\n",
      "Average ROUGE-2 Score: 0.0314\n",
      "Average ROUGE-L Score: 0.0733\n",
      "Average METEOR Score: 0.0348\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sacrebleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('predictions.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract original and predicted answers\n",
    "original_answers = [item['Ground Truth'] for item in data]\n",
    "predicted_answers = [item['Predicted'] for item in data]\n",
    "\n",
    "# Cosine Similarity\n",
    "vectorizer = TfidfVectorizer().fit_transform(original_answers + predicted_answers)\n",
    "vectors = vectorizer.toarray()\n",
    "cosine_scores = [cosine_similarity([vectors[i]], [vectors[i + len(original_answers)]])[0][0] for i in range(len(original_answers))]\n",
    "\n",
    "# BLEU Score\n",
    "bleu_scores = [sacrebleu.sentence_bleu(pred, [orig]).score for orig, pred in zip(original_answers, predicted_answers)]\n",
    "\n",
    "# CIDEr Score\n",
    "cider = Cider()\n",
    "cider_scores, _ = cider.compute_score({i: [pred] for i, pred in enumerate(predicted_answers)}, \n",
    "                                       {i: [orig] for i, orig in enumerate(original_answers)})\n",
    "\n",
    "# ROUGE Scores\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for orig, pred in zip(original_answers, predicted_answers):\n",
    "    scores = rouge.score(orig, pred)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# METEOR Score using Hugging Face's evaluate library\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "meteor_scores = meteor.compute(predictions=predicted_answers, references=original_answers)\n",
    "\n",
    "# Calculate overall scores\n",
    "overall_cosine_similarity = sum(cosine_scores) / len(cosine_scores)\n",
    "overall_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "overall_cider_score = cider_scores  # Directly use the CIDEr score\n",
    "overall_rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n",
    "overall_rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n",
    "overall_rougeL_score = sum(rougeL_scores) / len(rougeL_scores)\n",
    "overall_meteor_score = meteor_scores['meteor']  # Use the METEOR score\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Scores:\")\n",
    "print(f\"Average Cosine Similarity: {overall_cosine_similarity:.4f}\")\n",
    "print(f\"Average BLEU Score: {overall_bleu_score:.4f}\")\n",
    "print(f\"Average CIDEr Score: {overall_cider_score:.4f}\")\n",
    "print(f\"Average ROUGE-1 Score: {overall_rouge1_score:.4f}\")\n",
    "print(f\"Average ROUGE-2 Score: {overall_rouge2_score:.4f}\")\n",
    "print(f\"Average ROUGE-L Score: {overall_rougeL_score:.4f}\")\n",
    "print(f\"Average METEOR Score: {overall_meteor_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
